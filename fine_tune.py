import alogos as al
import jax.numpy as jnp
from jax import random, clear_caches
import numpy as np
import sys
import gc
import os
import time
import traceback
from config import Config as config
from model import NGCTransformer
from ngclearn.utils.metric_utils import measure_CatNLL
from data_preprocess.data_loader import DataLoader
from eval import eval_model
# --- 1. Environment & Setup ---
os.environ["PYTHONUNBUFFERED"] = "1"
os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"


LOG_FILE = "search_progress.log"
RESULT_FILE = "tuning_results.txt"

def log_message(message, end="\n"):
    print(message, end=end, flush=True)
    with open(LOG_FILE, "a") as f:
        f.write(message + end)

with open(LOG_FILE, "w") as f:
    f.write("=== New Search Session ===\n")

# --- 2. Grammar Guided Search Space ---
# WE USE PAIRED CONFIGURATIONS TO FORCE n_embed % n_heads == 0
# The Genetic Algorithm will select one valid string from this grammar.
# It will NOT try invalid combinations (like embed=64, heads=5).

bnf_text = """
<hparams>     ::= <arch_pair> "," <block_conf> "," <depth_conf> "," <steps> "," <learn_rate> "," <drop> "," <bounds>

# PAIRED: n_embed AND n_heads together to ensure divisibility
<arch_pair>   ::= "n_embed=64,n_heads=4" | "n_embed=128,n_heads=4" | "n_embed=128,n_heads=8" | "n_embed=256,n_heads=8"

# Block Size (affects DataLoader directly)
<block_conf>  ::= "block_size=64" | "block_size=128" | "block_size=256"

# Layers
<depth_conf>  ::= "n_layers=2" | "n_layers=4" | "n_layers=6"

# Other Hyperparameters
<steps>       ::= "T=5" | "T=10" | "T=15" | "T=20"
<learn_rate>  ::= "eta=0.001" | "eta=0.0005" | "eta=0.0001"
<drop>        ::= "dropout=0.1" | "dropout=0.2" | "dropout=0.3"
<bounds>      ::= "wlb=-0.05,wub=0.05"
"""

def get_dynamic_batch_size(n_embed, block_size):
    """
    Calculates batch size to maintain constant memory usage.
    As model size (n_embed) or data size (block_size) increases, batch size decreases.
    """
    # Heuristic: Total tokens in memory = batch * block * embed
    # Adjust the threshold (30000) based on your specific GPU VRAM
    complexity_score = n_embed * block_size
    
    if complexity_score >= 32000:  # e.g., 256 * 128
        return 8
    elif complexity_score >= 16000: # e.g., 128 * 128
        return 16
    elif complexity_score >= 8000:  # e.g., 64 * 128
        return 32
    else:
        return 64

# --- 3. The Objective Function (The "Trial") ---
def objective_function(phenotype_string):
    """
    This function represents ONE individual in the population.
    The GA generates the string, we build the model, train it, and return the score.
    """
    start_time = time.time()
    
    # Clean the string generated by the grammar
    clean_string = phenotype_string.replace('"', '').replace(' ', '')
    
    # Parse parameters into a dictionary
    try:
        params = {p.split('=')[0]: p.split('=')[1] for p in clean_string.split(',')}
    except Exception as e:
        log_message(f"[!] Parsing Error: {clean_string}")
        return 1e9

    # Extract Key Params
    n_embed = int(params['n_embed'])
    n_heads = int(params['n_heads'])
    seq_len = int(params['block_size'])
    n_layers = int(params['n_layers'])
    T = int(params['T'])
    eta = float(params['eta'])
    dropout = float(params['dropout'])
    wlb = float(params['wlb'])
    wub = float(params['wub'])

    log_message(f"\n" + "="*60)
    log_message(f"[Trial Start] Config: {clean_string}")
    
    # --- DYNAMIC BATCH SIZING ---
    # We calculate batch size based on the NEW n_embed and block_size
    curr_batch_size = get_dynamic_batch_size(n_embed, seq_len)
    log_message(f" >> Dynamic Sizing: Seq_Len={seq_len} | Embed={n_embed} -> Batch_Size={curr_batch_size}")

    model = None
    try:
        # --- DYNAMIC DATA RELOADING ---
        # The DataLoader MUST be re-initialized here because 'seq_len' changed.
        # This prevents "Index out of range" errors.
        # Note: We do NOT pass n_embed to DataLoader (it only handles integer tokens),
        # but we DO pass the batch_size which depends on n_embed.
        
        loader = DataLoader(seq_len=seq_len, batch_size=curr_batch_size)
        train_loader, valid_loader, _ = loader.load_and_prepare_data()
        
        # --- MODEL INITIALIZATION ---
        dkey = random.PRNGKey(int(time.time()))
        model = NGCTransformer(
            dkey, 
            batch_size=curr_batch_size, 
            seq_len=seq_len, 
            n_embed=n_embed,
            vocab_size=config.vocab_size, 
            n_layers=n_layers, 
            n_heads=n_heads,
            T=T, 
            dt=1.0, 
            tau_m=config.tau_m, 
            act_fx=config.act_fx, 
            eta=eta,
            dropout_rate=dropout, 
            pos_learnable=config.pos_learnable,
            optim_type=config.optim_type,
            wub=wub, 
            wlb=wlb, 
            exp_dir="exp",
            model_name="ngc_evo_temp"
        )

        # --- TRAINING LOOP (SHORT TERM) ---
        log_message(f"\n {'Batch':<6} | {'Energy (EFE)':<14} | {'PPL':<12} | {'CL (Loss)':<12}")
        log_message(f" {'-'*52}")

        total_efe = 0.0
        total_ppl = 0.0
        batch_count = 0
        
        # We only run a subset of batches (e.g., 20) to quickly evaluate the "fitness" of this configuration.
        # If it's good, the GA will pick it and refine it.
        max_batches_per_trial = 20 
        
        train_iter = iter(train_loader)
        
        for i in range(max_batches_per_trial):
            try:
                batch = next(train_iter)
            except StopIteration:
                break

            inputs = batch[0][1] # Shape: (Batch, Seq_Len)
            targets = batch[1][1]
            
            # Formatting targets
            targets_onehot = jnp.eye(config.vocab_size)[targets]
            targets_flat = targets_onehot.reshape(-1, config.vocab_size)
            
            # Forward + Update
            # adapt_synapses=True means we ARE training/optimizing weights here
            yMu_inf, _, _EFE = model.process(obs=inputs, lab=targets_flat, adapt_synapses=True)
            
            # Metrics
            y_pred = yMu_inf.reshape(-1, config.vocab_size)
            
            # --- THE "KILL SWITCH" ---
            # If gradients explode (NaN) or loss is absurdly high, kill immediately.
            if jnp.isnan(_EFE) or jnp.isnan(y_pred).any():
                log_message(f" [!] Trial Killed: NaN detected at batch {i}")
                return 1e9 # Maximum penalty
                
            batch_ce = float(measure_CatNLL(y_pred, targets_flat).mean())
            batch_ppl = float(np.exp(batch_ce))
            batch_efe = float(_EFE)
            
            log_message(f" {i:<6} | {batch_efe:<14.4f} | {batch_ppl:<12.2f} | {batch_ce:<12.4f}")
            
            total_efe += batch_efe
            total_ppl += batch_ppl
            batch_count += 1

        # Calculate Averages
        avg_efe = total_efe / batch_count if batch_count > 0 else 0
        avg_ppl = total_ppl / batch_count if batch_count > 0 else 0
        eval_time = time.time() - start_time

        log_message(f"\n[Summary] Avg Energy: {avg_efe:.4f} | Avg PPL: {avg_ppl:.2f} | Time: {eval_time:.2f}s")

        # --- FINAL VALIDATION ---
        # This return value is what the Genetic Algorithm uses to decide if this "individual" survives.
        val_ce, _ = eval_model(model, valid_loader, config.vocab_size)
        log_message(f"[Result] Validation Score (CE): {val_ce:.4f}")
        
        return float(val_ce)

    except KeyboardInterrupt:
        return 1e9
    except Exception as e:
        log_message(f"\n[!] ERROR in Trial: {e}")
        # traceback.print_exc() # Uncomment to see full error stack
        return 1e9 # Return high penalty so GA discards this config
    finally:
        # Crucial: Clear memory so the next trial starts fresh
        if model: del model
        clear_caches()
        gc.collect()

# --- 4. The Genetic Algorithm Execution ---
def main():
    # Load Grammar
    grammar = al.Grammar(bnf_text=bnf_text)
    
    # Define Evolutionary Algorithm
    # - generation_model="Generational": Standard evolution (Children replace parents)
    # - selection="Tournament": Good balance of exploration/exploitation
    ea = al.EvolutionaryAlgorithm(
        grammar, 
        objective_function, 
        'min', 
        population_size=10,   # Small population to keep it fast
        max_generations=5,    # Run for 5 generations
        verbose=True
    )
    
    log_message("\n" + "#"*50)
    log_message("STARTING GRAMMAR-GUIDED EVOLUTION")
    log_message("Optimizing for: Minimized Validation Cross-Entropy")
    log_message("#"*50)
    
    try:
        # .run() starts the evolution:
        # 1. Create random population
        # 2. Evaluate all
        # 3. Select best
        # 4. Crossover & Mutate
        # 5. Repeat
        best_ind = ea.run()
        
        # --- SAVE RESULTS ---
        log_message("\n" + "#"*50)
        log_message("OPTIMIZATION COMPLETED")
        log_message("#"*50)
        
        report = f"""
Optimization Success.

Best Fitness (Validation CE): {best_ind.fitness:.5f}
Best Parameters Found:
{best_ind.phenotype}

Method: Grammar-Guided Genetic Algorithm
"""
        print(report)
        with open(RESULT_FILE, "w") as f:
            f.write(report)
            
    except KeyboardInterrupt:
        print("\nOptimization stopped manually.")

if __name__ == "__main__":
    main()